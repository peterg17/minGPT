{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cute little demo showing the simplest usage of minGPT. Configured to run fine on Macbook Air in like a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchdata\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchtext.datasets\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.bpe import get_encoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "  Robert\n",
      "1285073\n"
     ]
    }
   ],
   "source": [
    "# train, validation, test = torchtext.datasets.WikiText103(root='./data')\n",
    "# print(\"train: \", list(train)[:10])\n",
    "wiki_text = open('wikitext-103-raw/wiki.test.raw', encoding='utf-8').read() \n",
    "wiki_text = wiki_text.replace(\"=\", \"\")\n",
    "print(wiki_text[:10])\n",
    "print(len(wiki_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    \"\"\" \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        self.MASK_CHAR = u\"\\u003D\" # the equals character\n",
    "        self.PAD_CHAR = u\"\\u25A1\" # empty square character for padding\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = len(chars)\n",
    "        print(\"vocab size is: \" + str(self.vocab_size))\n",
    "        self.data_size = len(data)\n",
    "        # self.data = list(data.encode('utf-8').decode('ascii', errors='ignore').split('\\n'))\n",
    "        self.data = data\n",
    "        \n",
    "#     @staticmethod\n",
    "#     def encode(b: bytes) -> int:\n",
    "#         return int.from_bytes(b, byteorder='big')\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def decode(i: int) -> bytes:\n",
    "#         return i.to_bytes(((i.bit_length() + 7) // 8), byteorder='big')               \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "#     def get_vocab_size(self):\n",
    "#         # https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "#         return 267735\n",
    "    \n",
    "#     def get_block_size(self):\n",
    "#         return 128\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "block_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is: 258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 1,  0,  1,  1, 50, 76, 63, 66, 79, 81,  1, 34, 76, 82, 73, 81, 66, 79,\n",
       "          1,  1,  0,  1,  0,  1, 50, 76, 63, 66, 79, 81,  1, 34, 76, 82, 73, 81,\n",
       "         66, 79,  1, 70, 80,  1, 62, 75,  1, 37, 75, 68, 73, 70, 80, 69,  1, 67,\n",
       "         70, 73, 74,  1, 13,  1, 81, 66, 73, 66, 83, 70, 80, 70, 76, 75,  1, 62,\n",
       "         75, 65,  1, 81, 69, 66, 62, 81, 79, 66,  1, 62, 64, 81, 76, 79,  1, 15,\n",
       "          1, 40, 66,  1, 69, 62, 65,  1, 62,  1, 68, 82, 66, 80, 81,  1, 32, 14,\n",
       "         32,  1, 80, 81, 62, 79, 79, 70, 75, 68,  1, 79, 76, 73, 66,  1, 76, 75,\n",
       "          1, 81, 69, 66,  1, 81, 66, 73, 66, 83, 70, 80, 70, 76, 75,  1, 80, 66,\n",
       "         79, 70, 66, 80,  1, 52, 69, 66,  1, 34, 70, 73, 73,  1, 70, 75,  1, 19,\n",
       "         17, 17, 17,  1, 15,  1, 52, 69, 70, 80,  1, 84, 62, 80,  1, 67, 76, 73,\n",
       "         73, 76, 84, 66, 65,  1, 63, 86,  1, 62,  1, 80, 81, 62, 79, 79, 70, 75,\n",
       "         68,  1, 79, 76, 73, 66,  1, 70, 75,  1, 81, 69, 66,  1, 77, 73, 62, 86,\n",
       "          1, 40, 66, 79, 76, 75, 80,  1, 84, 79, 70, 81, 81, 66, 75,  1, 63, 86,\n",
       "          1, 51, 70, 74, 76, 75,  1, 51, 81, 66, 77, 69, 66, 75, 80,  1, 13,  1,\n",
       "         84, 69, 70, 64, 69,  1, 84, 62, 80,  1, 77, 66, 79, 67, 76, 79, 74, 66,\n",
       "         65,  1, 70, 75,  1, 19, 17, 17, 18,  1, 62, 81,  1, 81, 69, 66,  1, 50,\n",
       "         76, 86, 62, 73,  1, 35, 76, 82, 79, 81,  1, 52, 69, 66, 62, 81, 79, 66,\n",
       "          1, 15,  1, 40, 66,  1, 69, 62, 65,  1, 62,  1, 68, 82, 66, 80, 81,  1,\n",
       "         79, 76, 73, 66,  1, 70, 75,  1, 81, 69, 66,  1, 81, 66, 73, 66, 83, 70,\n",
       "         80, 70, 76, 75,  1, 80, 66, 79, 70, 66, 80,  1, 42, 82, 65, 68, 66,  1,\n",
       "         42, 76, 69, 75,  1, 36, 66, 66, 65,  1, 70, 75,  1, 19, 17, 17, 19,  1,\n",
       "         15,  1, 41, 75,  1, 19, 17, 17, 21,  1, 34, 76, 82, 73, 81, 66, 79,  1,\n",
       "         73, 62, 75, 65, 66, 65,  1, 62,  1, 79, 76, 73, 66,  1, 62, 80,  1,  3,\n",
       "          1, 35, 79, 62, 70, 68,  1,  3,  1, 70, 75,  1, 81, 69, 66,  1, 66, 77,\n",
       "         70, 80, 76, 65, 66,  1,  3,  1, 52, 66, 65, 65, 86,  1,  8, 80,  1, 51,\n",
       "         81, 76, 79, 86,  1,  3,  1, 76, 67,  1, 81, 69, 66,  1, 81, 66, 73, 66,\n",
       "         83, 70, 80, 70, 76, 75,  1, 80, 66, 79, 70, 66, 80,  1, 52, 69, 66,  1,\n",
       "         44, 76, 75, 68,  1, 38, 70, 79, 74,  1, 28,  1, 69, 66]),\n",
       " tensor([ 0,  1,  1, 50, 76, 63, 66, 79, 81,  1, 34, 76, 82, 73, 81, 66, 79,  1,\n",
       "          1,  0,  1,  0,  1, 50, 76, 63, 66, 79, 81,  1, 34, 76, 82, 73, 81, 66,\n",
       "         79,  1, 70, 80,  1, 62, 75,  1, 37, 75, 68, 73, 70, 80, 69,  1, 67, 70,\n",
       "         73, 74,  1, 13,  1, 81, 66, 73, 66, 83, 70, 80, 70, 76, 75,  1, 62, 75,\n",
       "         65,  1, 81, 69, 66, 62, 81, 79, 66,  1, 62, 64, 81, 76, 79,  1, 15,  1,\n",
       "         40, 66,  1, 69, 62, 65,  1, 62,  1, 68, 82, 66, 80, 81,  1, 32, 14, 32,\n",
       "          1, 80, 81, 62, 79, 79, 70, 75, 68,  1, 79, 76, 73, 66,  1, 76, 75,  1,\n",
       "         81, 69, 66,  1, 81, 66, 73, 66, 83, 70, 80, 70, 76, 75,  1, 80, 66, 79,\n",
       "         70, 66, 80,  1, 52, 69, 66,  1, 34, 70, 73, 73,  1, 70, 75,  1, 19, 17,\n",
       "         17, 17,  1, 15,  1, 52, 69, 70, 80,  1, 84, 62, 80,  1, 67, 76, 73, 73,\n",
       "         76, 84, 66, 65,  1, 63, 86,  1, 62,  1, 80, 81, 62, 79, 79, 70, 75, 68,\n",
       "          1, 79, 76, 73, 66,  1, 70, 75,  1, 81, 69, 66,  1, 77, 73, 62, 86,  1,\n",
       "         40, 66, 79, 76, 75, 80,  1, 84, 79, 70, 81, 81, 66, 75,  1, 63, 86,  1,\n",
       "         51, 70, 74, 76, 75,  1, 51, 81, 66, 77, 69, 66, 75, 80,  1, 13,  1, 84,\n",
       "         69, 70, 64, 69,  1, 84, 62, 80,  1, 77, 66, 79, 67, 76, 79, 74, 66, 65,\n",
       "          1, 70, 75,  1, 19, 17, 17, 18,  1, 62, 81,  1, 81, 69, 66,  1, 50, 76,\n",
       "         86, 62, 73,  1, 35, 76, 82, 79, 81,  1, 52, 69, 66, 62, 81, 79, 66,  1,\n",
       "         15,  1, 40, 66,  1, 69, 62, 65,  1, 62,  1, 68, 82, 66, 80, 81,  1, 79,\n",
       "         76, 73, 66,  1, 70, 75,  1, 81, 69, 66,  1, 81, 66, 73, 66, 83, 70, 80,\n",
       "         70, 76, 75,  1, 80, 66, 79, 70, 66, 80,  1, 42, 82, 65, 68, 66,  1, 42,\n",
       "         76, 69, 75,  1, 36, 66, 66, 65,  1, 70, 75,  1, 19, 17, 17, 19,  1, 15,\n",
       "          1, 41, 75,  1, 19, 17, 17, 21,  1, 34, 76, 82, 73, 81, 66, 79,  1, 73,\n",
       "         62, 75, 65, 66, 65,  1, 62,  1, 79, 76, 73, 66,  1, 62, 80,  1,  3,  1,\n",
       "         35, 79, 62, 70, 68,  1,  3,  1, 70, 75,  1, 81, 69, 66,  1, 66, 77, 70,\n",
       "         80, 76, 65, 66,  1,  3,  1, 52, 66, 65, 65, 86,  1,  8, 80,  1, 51, 81,\n",
       "         76, 79, 86,  1,  3,  1, 76, 67,  1, 81, 69, 66,  1, 81, 66, 73, 66, 83,\n",
       "         70, 80, 70, 76, 75,  1, 80, 66, 79, 70, 66, 80,  1, 52, 69, 66,  1, 44,\n",
       "         76, 75, 68,  1, 38, 70, 79, 74,  1, 28,  1, 69, 66,  1]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = WikiDataset(wiki_text, block_size)\n",
    "x = train_dataset[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.12M\n"
     ]
    }
   ],
   "source": [
    "# create a GPT instance\n",
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "# from https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = block_size\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 100\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 5.56285\n",
      "iter_dt 1324.71ms; iter 10: train loss 5.08085\n",
      "iter_dt 1375.60ms; iter 20: train loss 4.71569\n",
      "iter_dt 1334.08ms; iter 30: train loss 4.37872\n",
      "iter_dt 1345.50ms; iter 40: train loss 4.07486\n",
      "iter_dt 1328.63ms; iter 50: train loss 3.83190\n",
      "iter_dt 1330.18ms; iter 60: train loss 3.61901\n",
      "iter_dt 1348.46ms; iter 70: train loss 3.44152\n",
      "iter_dt 1372.46ms; iter 80: train loss 3.30257\n",
      "iter_dt 1346.94ms; iter 90: train loss 3.18946\n"
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 10 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's perform some evaluation\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Boultereealesasshea ntesn sitedaernsesendtt  sio h hastrsoson hiarosietor  srnartes eie  seda r a atro inon ithilat  onrroaa tesriea tho hetito estit rt ienssn stal tasi eoiororo teseearhoeoinrnto h ro ededaitr arshei sss eiitolinee trea onrs ilannetitt tt oanr oeoisosretstoind  antoin tr  il etaat staeolsheis  halsarta rennnnoe sn teat soos anioeote tseeee htt hss otit  n  tee settr a hsr s shtta totiolissstttean e ner  t  et aresee iesn i ss ts eeriee rthe etoitinilalaieeinninialtedei insedi teos rit\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # sample from the model...\n",
    "    context = \"Robert Boulter\"\n",
    "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "    y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
    "    completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "    print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
